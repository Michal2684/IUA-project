{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d65bff-d4a1-4f0b-9606-922a55fa265a",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Phase 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fe9585e-c9fe-4d09-b88a-7e89482e4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "import statsmodels.stats as sm_stats\n",
    "import statsmodels.api as sm\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20772418-6579-45a0-b4d4-6d0d9df4be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_any(path):\n",
    "    try:\n",
    "        return pd.read_csv(path, sep=None, engine=\"python\", encoding=\"utf-8-sig\")\n",
    "    except Exception:\n",
    "        return pd.read_csv(path, sep=\";\", engine=\"python\", encoding=\"utf-8-sig\")\n",
    "\n",
    "patient = read_any(\"data/patient.csv\")\n",
    "station = read_any(\"data/station.csv\")\n",
    "observation = read_any(\"data/observation.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef8cf3-6d17-4588-a0eb-1c346516d4fe",
   "metadata": {},
   "source": [
    "## 2.1 Implementing Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7970796f-79df-4a4a-95fa-8fc08679f9df",
   "metadata": {},
   "source": [
    "# A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d228ae72-3d8c-4599-9017-735491450c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return series[(series >= lower) & (series <= upper)]\n",
    "\n",
    "\n",
    "low_outlier_columns = [\"SpO₂\",\"PI\",\"RR\",\"PRV\",\"Skin Temperature\",\"Motion/Activity index\",\"PVI\",\"Hb level\",\"SV\",\"Blood Flow Index\", \"PPG waveform features\",\"Signal Quality Index\", \"Respiratory effort\"]\n",
    "for column in low_outlier_columns:\n",
    "    data = observation[column]\n",
    "    data_without_outliers = remove_outliers_iqr(data)\n",
    "\n",
    "\n",
    "def removing_outliers_low_percentatge(df, columns):\n",
    "    cleaned_df = df.copy()\n",
    "    for col in columns:\n",
    "        if col in cleaned_df.columns:\n",
    "            Q1 = cleaned_df[col].quantile(0.25)\n",
    "            Q3 = cleaned_df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - 1.5 * IQR\n",
    "            upper = Q3 + 1.5 * IQR\n",
    "\n",
    "            before = len(cleaned_df)\n",
    "            cleaned_df = cleaned_df[(cleaned_df[col] >= lower) & (cleaned_df[col] <= upper)]\n",
    "            after = len(cleaned_df)\n",
    "\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36b1a34d-fee7-4511-8e73-d93f8854ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_clean = removing_outliers_low_percentatge(observation, low_outlier_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91e974f9-17c9-4ce1-824e-01fd5e93448f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11118, 23)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5318dd87-cd0a-48f1-89d6-790f01d80a8c",
   "metadata": {},
   "source": [
    "The cleaned dataset was divided into a training and testing set using an 80/20 ratio.\n",
    "The target variable is oximetry, representing a binary classification problem (critical vs. normal state).\n",
    "Stratified sampling was applied to preserve the class balance between both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd9d3e17-ce08-43e9-9e93-03a28cacfefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (8894, 22)\n",
      "Test set: (2224, 22)\n"
     ]
    }
   ],
   "source": [
    "X = observation_clean.drop(columns=['oximetry'])\n",
    "y = observation_clean['oximetry']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=11, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set:\", X_train.shape)\n",
    "print(\"Test set:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1872cae-38b0-4128-9f68-2619eff9a98f",
   "metadata": {},
   "source": [
    "# B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e39a7e5-92dd-4d43-a86c-f4cebaaf36e9",
   "metadata": {},
   "source": [
    "- The dataset observation_clean contains 23 numerical attributes and 11,118 records.\n",
    "- An initial inspection using .info() and .isna() confirms that there are no missing values in any of the columns.\n",
    "- All features are stored as float64, making the dataset already suitable for machine learning algorithms\n",
    "- Three columns: SpO₂ (is directly related to the target variable and could cause data leakage), latitude, and longitude (contain location metadata irrelevant for the classification task) were dropped from the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef565919-55cf-45c2-a0a3-8325407d3ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11118 entries, 0 to 12045\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   SpO₂                   11118 non-null  float64\n",
      " 1   HR                     11118 non-null  float64\n",
      " 2   PI                     11118 non-null  float64\n",
      " 3   RR                     11118 non-null  float64\n",
      " 4   EtCO₂                  11118 non-null  float64\n",
      " 5   FiO₂                   11118 non-null  float64\n",
      " 6   PRV                    11118 non-null  float64\n",
      " 7   BP                     11118 non-null  float64\n",
      " 8   Skin Temperature       11118 non-null  float64\n",
      " 9   Motion/Activity index  11118 non-null  float64\n",
      " 10  PVI                    11118 non-null  float64\n",
      " 11  Hb level               11118 non-null  float64\n",
      " 12  SV                     11118 non-null  float64\n",
      " 13  CO                     11118 non-null  float64\n",
      " 14  Blood Flow Index       11118 non-null  float64\n",
      " 15  PPG waveform features  11118 non-null  float64\n",
      " 16  Signal Quality Index   11118 non-null  float64\n",
      " 17  Respiratory effort     11118 non-null  float64\n",
      " 18  O₂ extraction ratio    11118 non-null  float64\n",
      " 19  SNR                    11118 non-null  float64\n",
      " 20  oximetry               11118 non-null  float64\n",
      " 21  latitude               11118 non-null  float64\n",
      " 22  longitude              11118 non-null  float64\n",
      "dtypes: float64(23)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "observation_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48a1f553-ff09-40c5-a899-f0293ebcddae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpO₂                     0\n",
       "HR                       0\n",
       "PI                       0\n",
       "RR                       0\n",
       "EtCO₂                    0\n",
       "FiO₂                     0\n",
       "PRV                      0\n",
       "BP                       0\n",
       "Skin Temperature         0\n",
       "Motion/Activity index    0\n",
       "PVI                      0\n",
       "Hb level                 0\n",
       "SV                       0\n",
       "CO                       0\n",
       "Blood Flow Index         0\n",
       "PPG waveform features    0\n",
       "Signal Quality Index     0\n",
       "Respiratory effort       0\n",
       "O₂ extraction ratio      0\n",
       "SNR                      0\n",
       "oximetry                 0\n",
       "latitude                 0\n",
       "longitude                0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(observation_clean.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a51fa1b4-94e3-4142-89f7-5689e1f92989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (11118, 20)\n",
      "Target vector shape: (11118,)\n"
     ]
    }
   ],
   "source": [
    "target = \"oximetry\"\n",
    "\n",
    "drop_cols = [\"SpO₂\", \"latitude\", \"longitude\"]\n",
    "\n",
    "X = observation_clean.drop(columns=[c for c in drop_cols if c in observation_clean.columns])\n",
    "y = observation_clean[target].astype(int)\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target vector shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6cb9125-7297-41e2-ad1d-e80780123fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X.isna().sum().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2211c2c1-4a58-4732-9ed9-bd73fc5c96ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final numeric dataset shape: (11118, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"Final numeric dataset shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b4471-b5f7-4e71-bb0a-dd967c2ffbf1",
   "metadata": {},
   "source": [
    "# C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1f8ff6-d7db-4f4d-8908-909790b89e0d",
   "metadata": {},
   "source": [
    "To prepare the data for machine learning, several feature transformation techniques were applied and evaluated.\n",
    "\n",
    "**Two main categories of preprocessing were tested together with a Logistic Regression classifier:**\n",
    "- Scaling methods: StandardScaler and MinMaxScaler\n",
    "- Transformers: PowerTransformer (Yeo–Johnson) and PCA (Principal Component Analysis)\n",
    "\n",
    "**Scaling Techniques**\n",
    "- Both scalers achieved nearly identical results, with accuracy around 0.85, precision around 0.84–0.85, and f1-score around 0.85.\n",
    "This indicates that the model performs similarly under both scaling methods and that the features are relatively well-behaved numerically.\n",
    "\n",
    "**Transformation Techniques**\n",
    "- The PowerTransformer achieved slightly better overall performance (accuracy = 0.85) compared to PCA (accuracy = 0.84).\n",
    "This suggests that normalizing skewed distributions was more effective than dimensionality reduction in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b337f57-4f41-4d5f-98c1-bea93a98cefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== StandardScaler ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.86      0.82       902\n",
      "         1.0       0.90      0.84      0.87      1322\n",
      "\n",
      "    accuracy                           0.85      2224\n",
      "   macro avg       0.84      0.85      0.84      2224\n",
      "weighted avg       0.85      0.85      0.85      2224\n",
      "\n",
      "=== MinMaxScaler ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.86      0.82       902\n",
      "         1.0       0.90      0.84      0.87      1322\n",
      "\n",
      "    accuracy                           0.85      2224\n",
      "   macro avg       0.84      0.85      0.84      2224\n",
      "weighted avg       0.85      0.85      0.85      2224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe_std = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipe_minmax = Pipeline([\n",
    "    ('scale', MinMaxScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipe_std.fit(X_train, y_train)\n",
    "pipe_minmax.fit(X_train, y_train)\n",
    "\n",
    "y_pred_std = pipe_std.predict(X_test)\n",
    "y_pred_minmax = pipe_minmax.predict(X_test)\n",
    "\n",
    "print(\"=== StandardScaler ===\")\n",
    "print(classification_report(y_test, y_pred_std))\n",
    "\n",
    "print(\"=== MinMaxScaler ===\")\n",
    "print(classification_report(y_test, y_pred_minmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3250131e-7cca-4ac3-b526-49c928fdf9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PowerTransformer ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.86      0.82       902\n",
      "         1.0       0.90      0.83      0.87      1322\n",
      "\n",
      "    accuracy                           0.85      2224\n",
      "   macro avg       0.84      0.85      0.84      2224\n",
      "weighted avg       0.85      0.85      0.85      2224\n",
      "\n",
      "=== PCA Transformer ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.86      0.81       902\n",
      "         1.0       0.90      0.83      0.86      1322\n",
      "\n",
      "    accuracy                           0.84      2224\n",
      "   macro avg       0.83      0.84      0.84      2224\n",
      "weighted avg       0.85      0.84      0.84      2224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe_power = Pipeline([\n",
    "    ('transform', PowerTransformer(method='yeo-johnson')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "\n",
    "pipe_pca = Pipeline([\n",
    "    ('scale', StandardScaler()),   \n",
    "    ('pca', PCA(n_components=5)),  \n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipe_power.fit(X_train, y_train)\n",
    "y_pred_power = pipe_power.predict(X_test)\n",
    "\n",
    "pipe_pca.fit(X_train, y_train)\n",
    "y_pred_pca = pipe_pca.predict(X_test)\n",
    "\n",
    "print(\"=== PowerTransformer ===\")\n",
    "print(classification_report(y_test, y_pred_power))\n",
    "\n",
    "print(\"=== PCA Transformer ===\")\n",
    "print(classification_report(y_test, y_pred_pca))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3786038a-3071-4482-8558-e0534ee9d366",
   "metadata": {},
   "source": [
    "# D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92802bf-77d3-40b1-b986-75130148cfef",
   "metadata": {},
   "source": [
    "**Data Selection and Structure**\n",
    "\n",
    "The analysis focused on the observation_clean dataset, which contained all relevant physiological attributes in numeric form.\n",
    "Files (station, patient) were excluded from the preprocessing pipeline because they contained data not essential for modeling oxygen saturation states.\n",
    "\n",
    "**Data Cleaning**\n",
    "\n",
    "An inspection with .info() and .isna() confirmed that the dataset had no missing values and all columns were of type float64.\n",
    "This eliminated the need for imputation or categorical encoding.\n",
    "\n",
    "**Feature Preparation**\n",
    "\n",
    "The target variable was defined as oximetry, representing a binary classification problem. SpO₂ (directly related to the target and would cause data leakage), latitude and longitude (provide only metadata without physiological meaning)\n",
    "were removed.\n",
    "\n",
    "**Scaling and Transformation**\n",
    "\n",
    "Four transformation approaches were tested:\n",
    "- StandardScaler – normalization to zero mean and unit variance.\n",
    "- MinMaxScaler – scaling to a [0, 1] range.\n",
    "- PowerTransformer (Yeo–Johnson) - was chosen to adjust for skewed distributions of numerical attributes, which allows linear models to better capture relationships in the data and reduce the impact of extreme values.\n",
    "- Principal Component Analysis - was used as a dimensionality reduction. It simplifies data by removing redundancy between correlated attributes. It creates a smaller dataset that still contains the essential information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305908a-f13e-4822-9e06-2aedbdc8193e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
